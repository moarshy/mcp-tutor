This module, "Core MCP Primitives: Context, Actions, and Interaction," delves into the fundamental building blocks that enable seamless and secure communication between clients, servers, and Large Language Models (LLMs) within the Model Context Protocol (MCP). Understanding these primitives is crucial for grasping how MCP facilitates sophisticated LLM applications.

The module defines and differentiates the core MCP primitives:

*   **MCP Resources**: These are the primary mechanism through which servers provide essential context to LLMs. Resources represent data, files, or any information relevant to an LLM's task, allowing servers to curate and deliver the necessary background for intelligent responses.
*   **MCP Tools**: Tools empower LLMs to perform real-world actions. Servers expose specific functionalities as Tools, which LLMs can invoke (with user approval) to interact with external systems, retrieve dynamic information, or execute operations, thereby extending their capabilities beyond text generation.
*   **MCP Prompts**: Prompts play a vital role in standardizing and guiding LLM interactions. They define the structure, intent, and constraints for LLM inputs and outputs, ensuring consistent and predictable behavior across different applications and use cases.
*   **MCP Roots**: Roots define the operational boundaries and access permissions for servers. They establish secure sandboxes, ensuring that servers operate within predefined limits and can only access authorized resources, thereby enhancing security and control.
*   **MCP Sampling**: This concept is critical for secure LLM completions. Sampling refers to the process by which the client securely obtains and processes LLM outputs, often involving techniques to ensure data integrity and prevent malicious injections or unintended behaviors.

By mastering these core primitives—Resources for context, Tools for actions, and Prompts, Roots, and Sampling for controlled interaction—you gain a comprehensive understanding of how MCP enables robust, secure, and highly functional LLM-powered applications.