{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1164cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbfb67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.getLogger(\"litellm\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "dspy.configure(lm=dspy.LM(\"gemini/gemini-2.5-flash\", max_tokens=20000, cache=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a114892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import frontmatter\n",
    "from enum import Enum\n",
    "\n",
    "# =============================================================================\n",
    "# Data Models (Simplified)\n",
    "# =============================================================================\n",
    "\n",
    "class DocumentType(str, Enum):\n",
    "    REFERENCE = \"reference\"\n",
    "    GUIDE = \"guide\"\n",
    "    API = \"api\"\n",
    "    EXAMPLE = \"example\"\n",
    "    OVERVIEW = \"overview\"\n",
    "    CONFIG = \"configuration\"\n",
    "    TROUBLESHOOTING = \"troubleshooting\"\n",
    "    CHANGELOG = \"changelog\"\n",
    "\n",
    "class ComplexityLevel(str, Enum):\n",
    "    BEGINNER = \"beginner\"\n",
    "    INTERMEDIATE = \"intermediate\"\n",
    "    ADVANCED = \"advanced\"\n",
    "\n",
    "class DocumentAnalysis(BaseModel):\n",
    "    \"\"\"Simplified document analysis result\"\"\"\n",
    "    file_path: str\n",
    "    title: str\n",
    "    doc_type: DocumentType\n",
    "    complexity_level: ComplexityLevel\n",
    "    key_concepts: List[str]\n",
    "    learning_objectives: List[str]\n",
    "    semantic_summary: str\n",
    "    code_languages: List[str]\n",
    "    headings: List[str]\n",
    "    prerequisites: List[str]\n",
    "    related_topics: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7332bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Agent Signatures (Simplified)\n",
    "# =============================================================================\n",
    "\n",
    "class BasicMetadataExtractor(dspy.Signature):\n",
    "    \"\"\"Extract basic metadata from document content\"\"\"\n",
    "    content: str = dspy.InputField(desc=\"Raw document content\")\n",
    "    filename: str = dspy.InputField(desc=\"Document filename\")\n",
    "    \n",
    "    title: str = dspy.OutputField(desc=\"Document title\")\n",
    "    headings: str = dspy.OutputField(desc=\"JSON list of document headings\")\n",
    "    code_languages: str = dspy.OutputField(desc=\"JSON list of programming languages found\")\n",
    "\n",
    "class DocumentClassifier(dspy.Signature):\n",
    "    \"\"\"Classify document type and complexity level\"\"\"\n",
    "    content: str = dspy.InputField(desc=\"Document content\")\n",
    "    title: str = dspy.InputField(desc=\"Document title\")\n",
    "    overview_context: str = dspy.InputField(desc=\"Overview context from the repository. This should provide a high-level overview of the project.\")\n",
    "    \n",
    "    doc_type: DocumentType = dspy.OutputField(desc=\"Document type classification\")\n",
    "    complexity_level: ComplexityLevel = dspy.OutputField(desc=\"Complexity level assessment\")\n",
    "\n",
    "class ConceptExtractor(dspy.Signature):\n",
    "    \"\"\"Extract key concepts and learning objectives from document\"\"\"\n",
    "    content: str = dspy.InputField(desc=\"Document content\")\n",
    "    doc_type: str = dspy.InputField(desc=\"Document type\")\n",
    "    title: str = dspy.InputField(desc=\"Document title\")\n",
    "    \n",
    "    key_concepts: str = dspy.OutputField(desc=\"JSON list of 3-5 key concepts\")\n",
    "    learning_objectives: str = dspy.OutputField(desc=\"JSON list of learning objectives\")\n",
    "\n",
    "class SemanticAnalyzer(dspy.Signature):\n",
    "    \"\"\"Generate semantic summary and analyze relationships\"\"\"\n",
    "    content: str = dspy.InputField(desc=\"Document content\")\n",
    "    key_concepts: str = dspy.InputField(desc=\"Key concepts found\")\n",
    "    doc_type: str = dspy.InputField(desc=\"Document type\")\n",
    "    \n",
    "    semantic_summary: str = dspy.OutputField(desc=\"5-7 sentence semantic summary\")\n",
    "    prerequisites: str = dspy.OutputField(desc=\"JSON list of prerequisites\")\n",
    "    related_topics: str = dspy.OutputField(desc=\"JSON list of related topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf2f39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Helper Functions\n",
    "# =============================================================================\n",
    "\n",
    "def extract_basic_metadata(content: str, filepath: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Extract basic metadata - similar to your existing ContentExtractor\"\"\"\n",
    "    try:\n",
    "        post = frontmatter.loads(content)\n",
    "        frontmatter_data = post.metadata\n",
    "        clean_content = post.content\n",
    "    except:\n",
    "        frontmatter_data = {}\n",
    "        clean_content = content\n",
    "    \n",
    "    title = extract_title(clean_content, frontmatter_data, filepath.name)\n",
    "    headings = extract_headings(clean_content)\n",
    "    code_blocks = extract_code_blocks(clean_content)\n",
    "    \n",
    "    # Get primary language\n",
    "    code_languages = list(set(block['language'] for block in code_blocks \n",
    "                            if block['language'] not in ['text', 'txt', '']))\n",
    "    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'headings': headings,\n",
    "        'code_languages': code_languages,\n",
    "        'frontmatter': frontmatter_data,\n",
    "        'clean_content': clean_content\n",
    "    }\n",
    "\n",
    "def extract_title(content: str, frontmatter_data: dict, filename: str) -> str:\n",
    "    \"\"\"Extract document title\"\"\"\n",
    "    if 'title' in frontmatter_data:\n",
    "        return frontmatter_data['title'].strip()\n",
    "    \n",
    "    h1_match = re.search(r'^# (.+)$', content, re.MULTILINE)\n",
    "    if h1_match:\n",
    "        return h1_match.group(1).strip()\n",
    "    \n",
    "    return filename.replace('.md', '').replace('.mdx', '').replace('_', ' ').replace('-', ' ').title().strip()\n",
    "\n",
    "def extract_headings(content: str) -> List[str]:\n",
    "    \"\"\"Extract all headings from content\"\"\"\n",
    "    headings = []\n",
    "    for match in re.finditer(r'^(#{1,6})\\s+(.+)$', content, re.MULTILINE):\n",
    "        hashes = match.group(1)\n",
    "        text = match.group(2).strip()\n",
    "        headings.append(f\"{hashes} {text}\")\n",
    "    return headings\n",
    "\n",
    "def extract_code_blocks(content: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Extract code blocks with language information\"\"\"\n",
    "    code_blocks = []\n",
    "    pattern = r'```(\\w+)?\\n(.*?)\\n```'\n",
    "    for match in re.finditer(pattern, content, re.DOTALL):\n",
    "        language = match.group(1) or 'text'\n",
    "        code_content = match.group(2).strip()\n",
    "        code_blocks.append({\n",
    "            'language': language,\n",
    "            'content': code_content\n",
    "        })\n",
    "    return code_blocks\n",
    "\n",
    "def safe_json_parse(json_str: str, fallback: list = None) -> list:\n",
    "    \"\"\"Safely parse JSON string with fallback\"\"\"\n",
    "    if fallback is None:\n",
    "        fallback = []\n",
    "    try:\n",
    "        result = json.loads(json_str)\n",
    "        return result if isinstance(result, list) else fallback\n",
    "    except:\n",
    "        # Try to parse as comma-separated string\n",
    "        if isinstance(json_str, str) and json_str.strip():\n",
    "            return [item.strip() for item in json_str.split(',') if item.strip()]\n",
    "        return fallback\n",
    "\n",
    "\n",
    "def get_n_words(text: str, n: int) -> str:\n",
    "    \"\"\"Get the first n words from a text\"\"\"\n",
    "    return ' '.join(text.split()[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7c976f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Main Document Analyzer\n",
    "# =============================================================================\n",
    "\n",
    "class DocumentAnalyzer(dspy.Module):\n",
    "    \"\"\"Simplified document analyzer with focused multi-agent approach\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metadata_extractor = dspy.ChainOfThought(BasicMetadataExtractor)\n",
    "        self.classifier = dspy.ChainOfThought(DocumentClassifier)\n",
    "        self.concept_extractor = dspy.ChainOfThought(ConceptExtractor)\n",
    "        self.semantic_analyzer = dspy.ChainOfThought(SemanticAnalyzer)\n",
    "\n",
    "        # configs\n",
    "        self.max_overview_words = 10000\n",
    "        self.max_content_words = 20000\n",
    "    \n",
    "    def analyze_document(self, file_path: str, overview_context: str = \"\") -> DocumentAnalysis:\n",
    "        \"\"\"Analyze a single document with multi-agent approach\"\"\"\n",
    "        \n",
    "        # Read document\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        filepath = Path(file_path)\n",
    "        \n",
    "        # Extract basic metadata (similar to your existing approach)\n",
    "        basic_data = extract_basic_metadata(content, filepath)\n",
    "        \n",
    "        # Step 1: Enhanced metadata extraction (only if basic data is incomplete)\n",
    "        needs_enhanced_extraction = (\n",
    "            not basic_data['title'] or \n",
    "            not basic_data['headings'] or \n",
    "            not basic_data['code_languages']\n",
    "        )\n",
    "        \n",
    "        if needs_enhanced_extraction:\n",
    "            try:\n",
    "                metadata_result = self.metadata_extractor(\n",
    "                    content=get_n_words(content, self.max_content_words),\n",
    "                    filename=filepath.name\n",
    "                )\n",
    "                \n",
    "                # Merge with basic extraction\n",
    "                title = metadata_result.title or basic_data['title']\n",
    "                headings = safe_json_parse(metadata_result.headings, basic_data['headings'])\n",
    "                code_languages = safe_json_parse(metadata_result.code_languages, basic_data['code_languages'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in metadata extraction: {e}\")\n",
    "                # Fallback to basic extraction\n",
    "                title = basic_data['title']\n",
    "                headings = basic_data['headings']\n",
    "                code_languages = basic_data['code_languages']\n",
    "        else:\n",
    "            # Use basic extraction results directly\n",
    "            title = basic_data['title']\n",
    "            headings = basic_data['headings']\n",
    "            code_languages = basic_data['code_languages']\n",
    "        \n",
    "        # Step 2: Classification\n",
    "        try:\n",
    "            classification_result = self.classifier(\n",
    "                content=get_n_words(content, self.max_content_words),  # First 2000 chars\n",
    "                title=title,\n",
    "                overview_context=get_n_words(overview_context, self.max_overview_words)\n",
    "            )\n",
    "            doc_type = classification_result.doc_type\n",
    "            complexity_level = classification_result.complexity_level\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in classification: {e}\")\n",
    "            # Fallback classification\n",
    "            doc_type = DocumentType.GUIDE\n",
    "            complexity_level = ComplexityLevel.INTERMEDIATE\n",
    "        \n",
    "        # Step 3: Concept extraction\n",
    "        try:\n",
    "            concept_result = self.concept_extractor(\n",
    "                content=get_n_words(content, self.max_content_words),\n",
    "                doc_type=doc_type.value,\n",
    "                title=title\n",
    "            )\n",
    "            key_concepts = safe_json_parse(concept_result.key_concepts)\n",
    "            learning_objectives = safe_json_parse(concept_result.learning_objectives)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in concept extraction: {e}\")\n",
    "            key_concepts = []\n",
    "            learning_objectives = []\n",
    "        \n",
    "        # Step 4: Semantic analysis\n",
    "        try:\n",
    "            semantic_result = self.semantic_analyzer(\n",
    "                content=get_n_words(content, self.max_content_words),\n",
    "                key_concepts=json.dumps(key_concepts),\n",
    "                doc_type=doc_type.value\n",
    "            )\n",
    "            semantic_summary = semantic_result.semantic_summary\n",
    "            prerequisites = safe_json_parse(semantic_result.prerequisites)\n",
    "            related_topics = safe_json_parse(semantic_result.related_topics)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in semantic analysis: {e}\")\n",
    "            semantic_summary = f\"Documentation about {title}\"\n",
    "            prerequisites = []\n",
    "            related_topics = []\n",
    "        \n",
    "        # Create analysis result\n",
    "        return DocumentAnalysis(\n",
    "            file_path=file_path,\n",
    "            title=title,\n",
    "            doc_type=doc_type,\n",
    "            complexity_level=complexity_level,\n",
    "            key_concepts=key_concepts,\n",
    "            learning_objectives=learning_objectives,\n",
    "            semantic_summary=semantic_summary,\n",
    "            code_languages=code_languages,\n",
    "            headings=headings,\n",
    "            prerequisites=prerequisites,\n",
    "            related_topics=related_topics\n",
    "        )\n",
    "    \n",
    "    def analyze_batch(self, file_paths: List[str], overview_context: str = \"\") -> List[DocumentAnalysis]:\n",
    "        \"\"\"Analyze multiple documents\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                analysis = self.analyze_document(file_path, overview_context)\n",
    "                results.append(analysis)\n",
    "                logger.info(f\"✓ Analyzed: {Path(file_path).name}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"✗ Failed to analyze {Path(file_path).name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaf41bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/arshath/play/naptha/tutor/.cache/modelcontextprotocol_docs_9b06b34c6341a02b233055dc593dd641/docs/concepts/architecture.mdx', 'r') as f:\n",
    "    overview_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65e3faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_analyzer = DocumentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40f32a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = '/Users/arshath/play/naptha/tutor/.cache/modelcontextprotocol_docs_9b06b34c6341a02b233055dc593dd641/docs/concepts/prompts.mdx'\n",
    "doc_analysis = document_analyzer.analyze_document(\n",
    "    file_path=test_file,\n",
    "    overview_context=\"overview_content\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0943d6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': '/Users/arshath/play/naptha/tutor/.cache/modelcontextprotocol_docs_9b06b34c6341a02b233055dc593dd641/docs/concepts/prompts.mdx',\n",
       " 'title': 'Prompts',\n",
       " 'doc_type': <DocumentType.GUIDE: 'guide'>,\n",
       " 'complexity_level': <ComplexityLevel.INTERMEDIATE: 'intermediate'>,\n",
       " 'key_concepts': ['Prompt Templates',\n",
       "  'Prompt Structure and Arguments',\n",
       "  'Client-Server Interaction for Prompts',\n",
       "  'Dynamic Prompt Capabilities',\n",
       "  'Prompt Security and Best Practices'],\n",
       " 'learning_objectives': ['Define what prompts are within the Model Context Protocol (MCP) and explain their primary purpose in standardizing LLM interactions.',\n",
       "  \"Describe the essential components of a prompt's structure, including its name, description, and the role of dynamic arguments.\",\n",
       "  'Explain the client-server interaction mechanisms for discovering available prompts (`prompts/list`) and executing specific prompts (`prompts/get`).',\n",
       "  'Illustrate how prompts can be dynamic by incorporating embedded resource context and supporting multi-step workflows.',\n",
       "  'Identify and apply best practices and security considerations for implementing robust, user-controlled, and secure prompt systems.',\n",
       "  'Recognize various ways prompts can be integrated into client user interfaces to enhance user interaction with LLMs.'],\n",
       " 'semantic_summary': 'Prompts in the Model Context Protocol (MCP) define reusable templates and workflows for interacting with Large Language Models (LLMs), designed to be user-controlled and easily surfaced by clients. These predefined templates can accept dynamic arguments, incorporate context from resources, chain multiple interactions, and guide specific workflows, often appearing as UI elements like slash commands. Clients discover available prompts via a `prompts/list` endpoint and utilize them through a `prompts/get` request, providing necessary arguments. Prompts support dynamic content, including embedded resource context and multi-step workflows, enabling complex interactions. Best practices emphasize clear naming, detailed descriptions, argument validation, and robust error handling, while security considerations highlight input sanitization, access controls, and prompt injection risks.',\n",
       " 'code_languages': ['json', 'typescript'],\n",
       " 'headings': ['## Overview',\n",
       "  '## Prompt structure',\n",
       "  '## Discovering prompts',\n",
       "  '## Using prompts',\n",
       "  '## Dynamic prompts',\n",
       "  '### Embedded resource context',\n",
       "  '### Multi-step workflows',\n",
       "  '## Example implementation',\n",
       "  '## Best practices',\n",
       "  '## UI integration',\n",
       "  '## Updates and changes',\n",
       "  '## Security considerations'],\n",
       " 'prerequisites': ['Understanding of client-server architecture',\n",
       "  'Familiarity with API concepts (requests, responses, endpoints)',\n",
       "  'Basic programming knowledge (e.g., TypeScript or Python for implementation examples)',\n",
       "  'Conceptual understanding of Large Language Models (LLMs) and their interaction patterns',\n",
       "  'Knowledge of JSON data structures'],\n",
       " 'related_topics': ['Large Language Models (LLMs)',\n",
       "  'API Design and Development',\n",
       "  'User Interface (UI) Integration',\n",
       "  'Workflow Automation',\n",
       "  'Security in AI Systems',\n",
       "  'Prompt Engineering',\n",
       "  'Software Development Kits (SDKs)',\n",
       "  'Context Management in AI']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_analysis.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9707b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ecf18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
